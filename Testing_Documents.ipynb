{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Testing Documents.ipynb",
      "provenance": [],
      "mount_file_id": "1tGPgnUMbpSmbbWRUqkaVUzOl3oCUiTfL",
      "authorship_tag": "ABX9TyOARPBqJe+lgu2VA3wRm1rA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yoshighosh/OCR-Testing/blob/main/Testing_Documents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdKoiNlpZ8Cx"
      },
      "source": [
        "#Preliminary Downloads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nM9W5DWaNF5"
      },
      "source": [
        "Tesseract Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcxf66-ZZ7um",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f96450a9-d0c8-4a09-e46b-88a59c1b3c94"
      },
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 2s (3,050 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 148492 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.8.tar.gz (14 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from pytesseract) (7.1.2)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.8-py2.py3-none-any.whl size=14072 sha256=de65967849b696faa08b6eee676633d4f60ea7eaab863c1cbae3ff7692383bfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/89/b9/3f11250225d0f90e5454fcc30fd1b7208db226850715aa9ace\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orkrQSCTbngd"
      },
      "source": [
        "PDF file downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nwf4q8P_UBca",
        "outputId": "ba855475-d233-44a5-f5e1-78e7cd36d8a0"
      },
      "source": [
        "!apt-get install poppler-utils "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 154 kB of archives.\n",
            "After this operation, 613 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 poppler-utils amd64 0.62.0-2ubuntu2.12 [154 kB]\n",
            "Fetched 154 kB in 1s (206 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 148539 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_0.62.0-2ubuntu2.12_amd64.deb ...\n",
            "Unpacking poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Setting up poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pawr5eMnaxnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718c371f-c881-4ebe-bdb9-a337ce827573"
      },
      "source": [
        "!pip install pdf2image\n",
        "\n",
        "!pip install python-poppler-qt5"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (7.1.2)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.0\n",
            "Collecting python-poppler-qt5\n",
            "  Downloading python-poppler-qt5-21.1.0.tar.gz (28 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/70/13/264e20336f7951044a8dd49cb1460bd318e2ebacff1753f908e6544ec933/python-poppler-qt5-21.1.0.tar.gz#sha256=add766db2c04026a6087f38f2044e66c8b053c81002f3753d8059713497d022d (from https://pypi.org/simple/python-poppler-qt5/). Command errored out with exit status 1: /usr/bin/python3 /usr/local/lib/python3.7/dist-packages/pip/_vendor/pep517/in_process/_in_process.py prepare_metadata_for_build_wheel /tmp/tmpsjvf86gh Check the logs for full command output.\u001b[0m\n",
            "  Downloading python-poppler-qt5-0.75.0.tar.gz (23 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/6a/7d/65a14ece5dd6a1564b576c1ca30b0f5639be64cc55b62b4d2b497159ed43/python-poppler-qt5-0.75.0.tar.gz#sha256=ea0ec9ebe995705ab19a301290365652e62bab5c9b05db5697c7bf2173335107 (from https://pypi.org/simple/python-poppler-qt5/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading python-poppler-qt5-0.74.0.tar.gz (23 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/c0/74/1479a3435ce3a68cb6f1de7cd60398c480893bcb5f6d7be2b7fa4c58f7a6/python-poppler-qt5-0.74.0.tar.gz#sha256=a21ad645d32c7544782587a01530378ff2718ad007a4ad5279a9783935a6086c (from https://pypi.org/simple/python-poppler-qt5/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading python-poppler-qt5-0.24.2.tar.gz (20 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/af/e7/aa451d4ca0910472c4442d8aa6ef44300852926d85ad033b029a22157027/python-poppler-qt5-0.24.2.tar.gz#sha256=3970c35ce1f0f1464a6c2746bea4c479b9780b4e17030c92479f7f1738a5c950 (from https://pypi.org/simple/python-poppler-qt5/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "  Downloading python-poppler-qt5-0.24.1.tar.gz (17 kB)\n",
            "\u001b[33mWARNING: Discarding https://files.pythonhosted.org/packages/26/41/81e7ee90baeeee2a5179af742edbac621ebf58d66b980f55547ef21e58f8/python-poppler-qt5-0.24.1.tar.gz#sha256=4254c4d5a1a76b13f8350815c90693bccee4705f4a1c8ad1021dd79f6b008c84 (from https://pypi.org/simple/python-poppler-qt5/). Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement python-poppler-qt5 (from versions: 0.24.1, 0.24.2, 0.74.0, 0.75.0, 21.1.0)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for python-poppler-qt5\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSIdZ2ZabqSI"
      },
      "source": [
        "PPTX file downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UIaSWKqbtJA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7fdcd2f-937a-4559-bdf2-f937d7ad4412"
      },
      "source": [
        "!pip install python-pptx"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-pptx\n",
            "  Downloading python-pptx-0.6.19.tar.gz (9.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.3 MB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from python-pptx) (4.2.6)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from python-pptx) (7.1.2)\n",
            "Collecting XlsxWriter>=0.5.7\n",
            "  Downloading XlsxWriter-3.0.1-py3-none-any.whl (148 kB)\n",
            "\u001b[K     |████████████████████████████████| 148 kB 50.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: python-pptx\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.19-py3-none-any.whl size=469952 sha256=6a102d14018a0d966b2f1253236b77b0d3b9c6387a0e6949b7613e4ad6a7f24c\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/2f/48/d23f362a5c4728f063b61c4be61e23c2415da1a371f902e5b4\n",
            "Successfully built python-pptx\n",
            "Installing collected packages: XlsxWriter, python-pptx\n",
            "Successfully installed XlsxWriter-3.0.1 python-pptx-0.6.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvHO8Wpcby9h"
      },
      "source": [
        "DOCX file downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7myiFD22b2Bv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6befd41-6b12-43c9-bf38-8d5de564cd74"
      },
      "source": [
        "!pip install python-docx"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from python-docx) (4.2.6)\n",
            "Building wheels for collected packages: python-docx\n",
            "  Building wheel for python-docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184508 sha256=961574cd6c5eb3c79bf83ed05beaa2aa9477079b09d678d0c9aa14f37ef92c87\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n",
            "Successfully built python-docx\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-0.8.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3X_qQGhaQrr"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pX5xvAncQ8T"
      },
      "source": [
        "#Import Required Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIH23xdjqgyQ"
      },
      "source": [
        "[[1.         0.03005134 0.01296408 0.         0.         0.03133414 0.03445396 0.03208275 0.44255733]\n",
        " [0.03005134 1.         0.00562574 0.         0.         0.03673798 0.08287585 0.06649821 0.        ]\n",
        " [0.01296408 0.00562574 1.         0.         0.         0.02335712 0.02277339 0.01615491 0.        ]\n",
        " [0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
        " [0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
        " [0.03133414 0.03673798 0.02335712 0.         0.         1.         0.1927276  0.02965571 0.        ]\n",
        " [0.03445396 0.08287585 0.02277339 0.         0.         0.1927276  1.         0.04533234 0.        ]\n",
        " [0.03208275 0.06649821 0.01615491 0.         0.         0.02965571 0.04533234 1.         0.        ]\n",
        " [0.44255733 0.         0.         0.         0.         0.         0.         0.         1.        ]]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQKLSvoRcUGL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ede55d7-3efc-4fa5-bf5f-360a61e11940"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow \n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "from nltk import ngrams, FreqDist\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('stopwords') \n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from pdf2image import convert_from_path\n",
        "from IPython.display import Image\n",
        "from pptx import Presentation\n",
        "import glob\n",
        "import docx"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esc0RB2DZ0vo"
      },
      "source": [
        "#Setting up OCR for each file type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ao-wNwLKc3me"
      },
      "source": [
        "Grayscale Function (Image and PDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ua50WDdjZ0Tw"
      },
      "source": [
        "def get_grayscale(image):\n",
        "  return cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCL-viB7dVvM"
      },
      "source": [
        "Image Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNqYfuN6dX_b"
      },
      "source": [
        "def getImageText(filename):\n",
        "  img = cv2.imread(filename)\n",
        "  grayscale = get_grayscale(img)\n",
        "  custom_config = r'-l eng --psm 6'\n",
        "  text = pytesseract.image_to_string(grayscale, config=custom_config)\n",
        "  return text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHXzMioTdz2P"
      },
      "source": [
        "PDF Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okyIxVj0d1kp"
      },
      "source": [
        "def getPDFText(filename):\n",
        "  pages = convert_from_path(filename, 500)\n",
        "  pagenumber = 1\n",
        "  page_paths = []\n",
        "  for page in pages:\n",
        "    filename = \"page\"+ str(pagenumber) + \".jpg\"\n",
        "    pagenumber += 1\n",
        "    page_paths.append(filename)\n",
        "    page.save(filename, 'JPEG')\n",
        "  images = []\n",
        "  for page in page_paths:\n",
        "    images.append(cv2.imread(page))\n",
        "  for image in images:\n",
        "    image = get_grayscale(image)\n",
        "  custom_config = r'-l eng --psm 6'\n",
        "  text = \"\"\n",
        "  for grayscale in images:\n",
        "    text += pytesseract.image_to_string(grayscale, config=custom_config)\n",
        "  return text\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_yiQs-pel9j"
      },
      "source": [
        "PPTX Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4wAxDr8eqpc"
      },
      "source": [
        "def getPPTXText(filename):\n",
        "  for eachfile in glob.glob(filename):\n",
        "      prs = Presentation(eachfile)\n",
        "      fullText = []\n",
        "      for slide in prs.slides:\n",
        "          for shape in slide.shapes:\n",
        "              if hasattr(shape, \"text\"):\n",
        "                  fullText.append(shape.text)\n",
        "  return '\\n'.join(fullText)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tod51qB9e_GN"
      },
      "source": [
        "DOCX Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzLk4i_SfAsa"
      },
      "source": [
        "def getDOCXText(filename):\n",
        "    doc = docx.Document(filename)\n",
        "    fullText = []\n",
        "    for para in doc.paragraphs:\n",
        "        fullText.append(para.text)\n",
        "    return '\\n'.join(fullText)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWclSckqZs-c"
      },
      "source": [
        "# Detecting file type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWjuJGo4ZCUt"
      },
      "source": [
        "def detectFileType(filename):\n",
        "  extension = filename[filename.rfind(\".\")+1:]\n",
        "  return extension\n",
        "\n",
        "def getFileName(filepath):\n",
        "  filename = filepath[filepath.rfind(\"/\")+1:]\n",
        "  return filename\n",
        "\n",
        "#read file binary, detect file\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAMXDek4TGvD"
      },
      "source": [
        "filepaths = []\n",
        "\n",
        "directory_name = \"/content/drive/MyDrive/Aroshi_highSchool/SEAP vounteer/Final PDFS/\"\n",
        "\n",
        "files = [\"10894.pdf\", \"12049.pdf\", \"13259.pdf\", \"16393.pdf\", \"18209.pdf\", \"19970.pdf\", \"20782.pdf\", \"23267.pdf\", \"23507.pdf\", \"23596.pdf\", \"25473.pdf\", \"287.pdf\", \"37632.pdf\", \"39172.pdf\", \"39955.pdf\", \"40879.pdf\", \"43032.pdf\", \"7183.pdf\", \"7502.pdf\", \"9307.pdf\"]\n",
        "\n",
        "for file in files:\n",
        "  filepaths.append(directory_name + file)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNHR5XjNg0hS"
      },
      "source": [
        "names = [\"A Safe, Efficient Regression Test Selection Technique1\", \"Occam's Razor: The Cutting Edge for Parser Technology\", \"Cone Trees in the UGA Graphics System: Suggestions of a more Robust Visualization Tool\", \"An Evaluation of Multiprocessor Cache Coherence Based on Virtual Memory Support\", \"Mutable Object State for Object-Oriented Logic Programming:\", \"A New Deterministic Parallel Sorting Algorithm With an Experimental Evaluation\", \"High Performance Geographic Information Systems:\", \"P++: A Language for Software System Generators?\", \"Models for Computer Generated Parody\", \"The Effect of Group Size and Communication Modes in CSCW Environments\", \"Extracting Multi-Dimensional Signal Features for Content-Based Visual Query\", \"Clustering Full Text Documents\", \"The Internet Software Visualization Laboratory\", \"Block Edit Models for Approximate String Matching?\", \"Structured Interviews on the Object-Oriented Paradigm\", \"Instance Pruning Techniques\", \"Observations and Recommendations on the Internationalisation of Software\", \"The Challenge of Deep Models, Inference Structures, and Abstract Tasks\", \"Using Introspective Reasoning to Select Learning Strategies\", \"Specifying and Adapting Object Behavior during System Evolution\"]\n",
        "IDs = [10894, 12049, 13259, 16393, 18209, 19970, 20782, 23267, 23507, 23596, 25473, 287, 37632, 39172, 39955, 40879, 43032, 7183, 7502, 9307]\n",
        "\n",
        "text = {}\n",
        "\n",
        "index = 0\n",
        "\n",
        "while index < 20:\n",
        "  text[IDs[index]] = names[index]\n",
        "  index += 1\n",
        "\n",
        "# for filepath in filepaths:\n",
        "#   file_type = detectFileType(filepath)\n",
        "#   filename = getFileName(filepath)\n",
        "#   string_text = \"\"\n",
        "#   if file_type == \"jpg\":\n",
        "#     string_text = getImageText(filepath)\n",
        "#   elif file_type == \"pdf\":\n",
        "#     string_text = getPDFText(filepath)\n",
        "#   elif file_type == \"pptx\":\n",
        "#     string_text = getPPTXText(filepath)\n",
        "#   elif file_type == \"docx\":\n",
        "#     string_text = getDOCXText(filepath)\n",
        "#   text[filename] = string_text\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rror2dFP3FYg",
        "outputId": "b00b5a0b-09aa-4a02-c40c-415cca1868a2"
      },
      "source": [
        "for file in text:\n",
        "  print(file)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10894\n",
            "12049\n",
            "13259\n",
            "16393\n",
            "18209\n",
            "19970\n",
            "20782\n",
            "23267\n",
            "23507\n",
            "23596\n",
            "25473\n",
            "287\n",
            "37632\n",
            "39172\n",
            "39955\n",
            "40879\n",
            "43032\n",
            "7183\n",
            "7502\n",
            "9307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OJK40q61UoN"
      },
      "source": [
        "#Preprocessing text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOzzPEPV19r0"
      },
      "source": [
        "def preprocessing(text):\n",
        "  words = word_tokenize(text)\n",
        "  updated_text = [word.lower() for word in words]\n",
        "  final_text = [x for x in updated_text if x.isalpha()]\n",
        "  stop_words = stopwords.words('english')\n",
        "  final_text = [word for word in final_text if word not in stop_words]\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  for word in final_text:\n",
        "    word = lemmatizer.lemmatize(word)\n",
        "  final_string = \" \".join(final_text)\n",
        "  return final_string"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WEs8AoO0LPsN",
        "outputId": "439facbc-817e-4cd9-d9d1-e5c507b853cc"
      },
      "source": [
        "query = input(\"Enter your query: \")\n",
        "print(query)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: Algorithm\n",
            "Algorithm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdEoI1MmLmp5"
      },
      "source": [
        "final_query = preprocessing(query)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8JPuSBz339o"
      },
      "source": [
        "for file in text:\n",
        "  text[file] = preprocessing(text[file])\n",
        "\n",
        "text[\"query\"] = final_query\n",
        "#text.pop(\"query\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMn73pADpbJU"
      },
      "source": [
        "df = pd.DataFrame()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be8pUuOV4CS5"
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([text[file] for file in text])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "#print(dense)\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "\n",
        "#display(df)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bp_wBYyYAuhS"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_sim = cosine_similarity(dense, dense)\n",
        "#print(np.matrix(cosine_sim))\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQptOXDeqNOM",
        "outputId": "09fff86a-6907-40ba-e30b-bcf6d7e9817a"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query = input(\"Enter your query: \")\n",
        "print(query)\n",
        "\n",
        "final_query = preprocessing(query)\n",
        "\n",
        "for file in text:\n",
        "  text[file] = preprocessing(text[file])\n",
        "\n",
        "text[\"query\"] = final_query\n",
        "#text.pop(\"query\")\n",
        "\n",
        "df = pd.DataFrame()\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([text[file] for file in text])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "#print(dense)\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "\n",
        "#display(df)\n",
        "\n",
        "cosine_sim = cosine_similarity(dense, dense)\n",
        "#print(np.matrix(cosine_sim))\n",
        "\n",
        "scores = cosine_sim[20]\n",
        "scores = scores[0:20]\n",
        "\n",
        "print(sorted(zip(scores, text), reverse=True)[:3])\n"
      ],
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: Unsupervised learning\n",
            "Unsupervised learning\n",
            "[(0.24219196229594228, 7502), (0.0, 43032), (0.0, 40879)]\n"
          ]
        }
      ]
    }
  ]
}